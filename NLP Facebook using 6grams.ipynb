{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin\\\\Documents\\\\NLP Project\\\\INLI@FIRE2017'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\Admin\\\\Documents\\\\NLP Project\\\\INLI@FIRE2017\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test_file = open('INLI_Train_Bengali_1','r')\n",
    "bengali = []\n",
    "hindi = []\n",
    "kannada = []\n",
    "malayalam = []\n",
    "tamil = []\n",
    "telugu = []\n",
    "for name in glob.glob(\"*.xml\"):\n",
    "    #print (name)\n",
    "    if \"BEN\" in name[11:]:\n",
    "        bengali.append(name)\n",
    "    elif \"HIN\" in name[11:]:\n",
    "        hindi.append(name)\n",
    "    elif \"KAN\" in name[11:]:\n",
    "        kannada.append(name)\n",
    "    elif \"MAL\" in name[11:]:\n",
    "        malayalam.append(name)\n",
    "    elif \"TAM\" in name[11:]:\n",
    "        tamil.append(name)\n",
    "    elif \"TEL\" in name[11:]:\n",
    "        telugu.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 202 200 203 207 210\n"
     ]
    }
   ],
   "source": [
    "print (len(hindi),len(bengali),len(malayalam),len(kannada),len(tamil),len(telugu))\n",
    "hindi_dataset = []\n",
    "bengali_dataset = []\n",
    "malay_dataset = []\n",
    "kannada_dataset = []\n",
    "tamil_dataset = []\n",
    "telugu_dataset = []\n",
    "grand_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 1233\n"
     ]
    }
   ],
   "source": [
    "print (\"Total = \" + str(len(hindi)+len(bengali)+len(malayalam)+len(kannada)+len(tamil)+len(telugu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in hindi:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    hindi_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('hindi',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in tamil:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    tamil_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('tamil',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in bengali:\n",
    "    f = open(f,'r')\n",
    "    try:\n",
    "        doc = minidom.parse(f)\n",
    "        bengali_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "        grand_list.append(('bengali',doc.documentElement.childNodes[5].firstChild.nodeValue))\n",
    "    except:\n",
    "        True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in telugu:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    telugu_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('telugu',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in kannada:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    kannada_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('kannada',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in malayalam:\n",
    "    f = open(f,'r')\n",
    "    try:\n",
    "        doc = minidom.parse(f)\n",
    "        malay_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "        grand_list.append(('malayalam',doc.documentElement.childNodes[5].firstChild.nodeValue))\n",
    "    except:\n",
    "        True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 211 201 203 210 196\n",
      "Total = 1228\n"
     ]
    }
   ],
   "source": [
    "print( len(tamil_dataset), len(hindi_dataset), len(bengali_dataset), len(kannada_dataset), len(telugu_dataset), len(malay_dataset))\n",
    "print (\"Total = \" + str(len(tamil_dataset) + len(hindi_dataset) + len(bengali_dataset) + len(kannada_dataset) + len(telugu_dataset) + len(malay_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tam = []\n",
    "for line in tamil_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    tam.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hin = []\n",
    "for line in hindi_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    hin.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ben = []\n",
    "for line in bengali_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    ben.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kan = []\n",
    "for line in kannada_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    kan.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mal = []\n",
    "for line in malay_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    mal.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tel = []\n",
    "for line in telugu_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    tel.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 211 201 203 210 196\n"
     ]
    }
   ],
   "source": [
    "print( len(tam), len(hin), len(ben), len(kan), len(tel), len(mal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32088\n"
     ]
    }
   ],
   "source": [
    "for sentence in tam:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('tamil',temp))\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59142\n"
     ]
    }
   ],
   "source": [
    "for sentence in hin:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('hindi',temp))\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95466\n"
     ]
    }
   ],
   "source": [
    "for sentence in ben:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('bengali',temp))\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138180\n"
     ]
    }
   ],
   "source": [
    "for sentence in kan:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('kannada',temp))\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183613\n"
     ]
    }
   ],
   "source": [
    "for sentence in tel:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('telugu',temp))\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228131\n"
     ]
    }
   ],
   "source": [
    "for sentence in mal:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('malayalam',temp))\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tamil', 'we support the people the police'),\n",
       " ('tamil', 'support the people the police should'),\n",
       " ('tamil', 'the people the police should be'),\n",
       " ('tamil', 'people the police should be trashed'),\n",
       " ('tamil', 'the police should be trashed out'),\n",
       " ('tamil', 'police should be trashed out of'),\n",
       " ('tamil', 'should be trashed out of the'),\n",
       " ('tamil', 'be trashed out of the department'),\n",
       " ('tamil', 'trashed out of the department u'),\n",
       " ('tamil', 'out of the department u r')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('malayalam', 'holes in our society against all'),\n",
       " ('malayalam', 'in our society against all norms'),\n",
       " ('malayalam', 'our society against all norms this'),\n",
       " ('malayalam', 'society against all norms this showing'),\n",
       " ('malayalam', 'against all norms this showing police'),\n",
       " ('malayalam', 'all norms this showing police and'),\n",
       " ('malayalam', 'norms this showing police and police'),\n",
       " ('malayalam', 'this showing police and police intelligence'),\n",
       " ('malayalam', 'showing police and police intelligence are'),\n",
       " ('malayalam', 'police and police intelligence are failure')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_list[len(ngram_list)-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tfidf_training_data(docs):\n",
    "    y = [d[0] for d in docs]\n",
    "\n",
    "    # Create the document corpus list\n",
    "    corpus = [d[1] for d in docs]\n",
    "\n",
    "    # Create the TF-IDF vectoriser and transform the corpus\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_tfidf_training_data(ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-c54e8a9a4a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "X_scaled = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=1000000.0, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-c24238b86b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'SVR' is not defined"
     ]
    }
   ],
   "source": [
    "model = SVR(cache_size=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(pred, y_test, labels=[\"hindi\", \"tamil\", \"bengali\", \"telugu\", \"kannada\", \"malayalam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
