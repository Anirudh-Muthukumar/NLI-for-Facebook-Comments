{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin\\\\Documents\\\\NLP Project\\\\INLI@FIRE2017'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\Admin\\\\Documents\\\\NLP Project\\\\INLI@FIRE2017\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test_file = open('INLI_Train_Bengali_1','r')\n",
    "bengali = []\n",
    "hindi = []\n",
    "kannada = []\n",
    "malayalam = []\n",
    "tamil = []\n",
    "telugu = []\n",
    "for name in glob.glob(\"*.xml\"):\n",
    "    #print (name)\n",
    "    if \"BEN\" in name[11:]:\n",
    "        bengali.append(name)\n",
    "    elif \"HIN\" in name[11:]:\n",
    "        hindi.append(name)\n",
    "    elif \"KAN\" in name[11:]:\n",
    "        kannada.append(name)\n",
    "    elif \"MAL\" in name[11:]:\n",
    "        malayalam.append(name)\n",
    "    elif \"TAM\" in name[11:]:\n",
    "        tamil.append(name)\n",
    "    elif \"TEL\" in name[11:]:\n",
    "        telugu.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 202 200 203 207 210\n"
     ]
    }
   ],
   "source": [
    "print (len(hindi),len(bengali),len(malayalam),len(kannada),len(tamil),len(telugu))\n",
    "hindi_dataset = []\n",
    "bengali_dataset = []\n",
    "malay_dataset = []\n",
    "kannada_dataset = []\n",
    "tamil_dataset = []\n",
    "telugu_dataset = []\n",
    "grand_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 1233\n"
     ]
    }
   ],
   "source": [
    "print (\"Total = \" + str(len(hindi)+len(bengali)+len(malayalam)+len(kannada)+len(tamil)+len(telugu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in hindi:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    hindi_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('hindi',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in tamil:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    tamil_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('tamil',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in bengali:\n",
    "    f = open(f,'r')\n",
    "    try:\n",
    "        doc = minidom.parse(f)\n",
    "        bengali_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "        grand_list.append(('bengali',doc.documentElement.childNodes[5].firstChild.nodeValue))\n",
    "    except:\n",
    "        True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in telugu:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    telugu_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('telugu',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in kannada:\n",
    "    f = open(f,'r')\n",
    "    doc = minidom.parse(f)\n",
    "    kannada_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "    grand_list.append(('kannada',doc.documentElement.childNodes[5].firstChild.nodeValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in malayalam:\n",
    "    f = open(f,'r')\n",
    "    try:\n",
    "        doc = minidom.parse(f)\n",
    "        malay_dataset.append(doc.documentElement.childNodes[5].firstChild.nodeValue)\n",
    "        grand_list.append(('malayalam',doc.documentElement.childNodes[5].firstChild.nodeValue))\n",
    "    except:\n",
    "        True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 211 201 203 210 196\n",
      "Total = 1228\n"
     ]
    }
   ],
   "source": [
    "print( len(tamil_dataset), len(hindi_dataset), len(bengali_dataset), len(kannada_dataset), len(telugu_dataset), len(malay_dataset))\n",
    "print (\"Total = \" + str(len(tamil_dataset) + len(hindi_dataset) + len(bengali_dataset) + len(kannada_dataset) + len(telugu_dataset) + len(malay_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tam = []\n",
    "for line in tamil_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    tam.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hin = []\n",
    "for line in hindi_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    hin.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ben = []\n",
    "for line in bengali_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    ben.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kan = []\n",
    "for line in kannada_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    kan.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mal = []\n",
    "for line in malay_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    mal.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tel = []\n",
    "for line in telugu_dataset:\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d\", \"\", line)\n",
    "    l=''\n",
    "    for ch in line :\n",
    "        if ch in exclude :\n",
    "            l+=' '\n",
    "        else:\n",
    "            l+=ch\n",
    "    tel.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 211 201 203 210 196\n"
     ]
    }
   ],
   "source": [
    "print( len(tam), len(hin), len(ben), len(kan), len(tel), len(mal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying bigrams classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_list = []\n",
    "tam_grams = []\n",
    "hin_grams = []\n",
    "ben_grams = []\n",
    "tel_grams = []\n",
    "kan_grams = []\n",
    "mal_grams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32916\n"
     ]
    }
   ],
   "source": [
    "for sentence in tam:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('tamil',temp))\n",
    "        tam_grams.append(temp)\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60814\n"
     ]
    }
   ],
   "source": [
    "for sentence in hin:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('hindi',temp))\n",
    "        hin_grams.append(temp)\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97942\n"
     ]
    }
   ],
   "source": [
    "for sentence in ben:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('bengali',temp))\n",
    "        ben_grams.append(temp)\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141468\n"
     ]
    }
   ],
   "source": [
    "for sentence in kan:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('kannada',temp))\n",
    "        kan_grams.append(temp)\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187741\n"
     ]
    }
   ],
   "source": [
    "for sentence in tel:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('telugu',temp))\n",
    "        tel_grams.append(temp)\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233043\n"
     ]
    }
   ],
   "source": [
    "for sentence in mal:\n",
    "    sixgrams = ngrams(sentence.split(),n)\n",
    "    for grams in sixgrams:\n",
    "        temp = \"\"\n",
    "        for g in grams:\n",
    "            temp = temp + g + \" \"\n",
    "        temp = temp[:len(temp)-1]\n",
    "        ngram_list.append(('malayalam',temp))\n",
    "        mal_grams.append(temp)\n",
    "print(len(ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233043"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tfidf_training_data(docs):\n",
    "    y = [d[0] for d in docs]\n",
    "\n",
    "    # Create the document corpus list\n",
    "    corpus = [d[1] for d in docs]\n",
    "\n",
    "    # Create the TF-IDF vectoriser and transform the corpus\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = create_tfidf_training_data(ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "start = time.time()\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(kernel='linear', probability=True, class_weight='auto'), max_samples=1.0 / n_estimators, n_estimators=n_estimators), n_jobs = -1)\n",
    "clf.fit(X_train, y_train)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.309940140316\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 697  234  362  208  280  237]\n",
      " [  81  354  121  158  178  177]\n",
      " [ 325  270  803  321  330  316]\n",
      " [1335 1914 1735 3615 2387 2371]\n",
      " [ 899  937  947 1236 1761 1176]\n",
      " [2175 2801 3409 3771 3722 4966]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(pred, y_test, labels=[\"hindi\", \"tamil\", \"bengali\", \"telugu\", \"kannada\", \"malayalam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=1000000.0, kernel='rbf')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(pred, y_test, labels=[\"hindi\", \"tamil\", \"bengali\", \"telugu\", \"kannada\", \"malayalam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
